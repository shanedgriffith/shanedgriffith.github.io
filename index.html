<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">

<head>
  <title>Shane Griffith. Ph.D.</title>
  <link rel="stylesheet" type="text/css" href="mystyle.css">
  <link rel="canonical" href="https://shanedgriffith.github.io" />
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-TY41FPJ49D"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', 'G-TY41FPJ49D');
  </script>
</head>

<body>
  <table class="background" width="100%" border="0" cellpadding="0" cellspacing="0">

    <tr>
      <td width="100%" align=" left" valign="top">
        <table class="background" width="100%" border="0" cellspacing="0" cellpadding="0">

          <!-- adds spacing to the top and left -->
          <tr>
            <!-- <br> -->
            <td width="40">&nbsp;</td>
            <!-- <td width="552">&nbsp;</td> -->
            <td width="552">&nbsp;</td>
          </tr>
          <tr>
            <!-- entry required because of the spacing -->
            <td>&nbsp;</td>

            <!-- finally, the content of the page.-->
            <td>
              <p> <!--keep this <p> so the heading is aligned with 'Home' -->
              <table class="background" cellspacing="0" cellpadding="0">
                <tr>
                  <td valign="top">
                    <div style="margin-top: 2px; margin-left: 1px;">
                      <img
                        style="border-left:15px solid black; border-right:15px solid black; border-color: #CDD2C5; width: 16.2Em"
                        src="images/self.png">
                    </div>
                  </td>
                  <td valign="top" class="maincontent">
                    <div style="margin-top: -1px; margin-left: 30px">
                      <font face="Garamond" style="font-size: 24pt">S<font face="Garamond" style="font-size: 22pt">HANE
                        </font>
                      </font>
                      <font face="Garamond" style="font-size: 24pt">G<font face="Garamond" style="font-size: 22pt"><!--
                          -->RIFFITH</font>
                      </font>

                      <!--<font face="Garamond" style="font-size: 24pt">   P</font><font face="Garamond" style="font-size: 22pt">H</font><font face="Garamond" style="font-size: 24pt">D</font> -->
                      <br>
                      <font face="Garamond" style="font-size: 12pt"> Ph.D. Georgia Tech.</font>
                      <br>
                      <font face="Garamond" style="font-size: 12pt"> M.S., B.S. Iowa State.</font>
                      <br>
                      <IMG style="height: 1.3em" SRC="./images/email.png"> <br><br>
                      <!-- <h5>LINKS</h5> -->
                      <ul class="social-icons">
                        <!-- <ul> -->
                        <li><a href="https://scholar.google.com/citations?user=urgfWQgAAAAJ&hl=en"><img width=25
                              src="images/icons/scholar.svg" alt="Google Scholar"></a></li>
                        <li><a href="https://www.linkedin.com/in/shane-griffith-4a4b054"><img width=25
                              src="images/icons/linkedin.svg" alt="LinkedIn"></a></li>
                        <li><a href="https://twitter.com/shanedgriffith"><img width=25 src="images/icons/twitter.svg"
                              alt="Twitter"></a></li>

                      </ul>
                      <p>
                        <!-- <h5>NEWS</h5> -->
                      <ul>

                    </div>

                  </td>
                </tr>
              </table>
            </td>
          </tr>
        </table>

    <tr>
      <td width="100%" align="left" valign="top">
        <table class="background" width="100%" border="0" cellspacing="0" cellpadding="0">

          <!-- determine the spacing of the two columns -->
          <tr>
            <td width="40">&nbsp;</td> <!-- An indent -->
            <td width="552">&nbsp;</td>
          </tr>
          <tr>
            <!-- entry required because of the spacing -->
            <td>&nbsp;</td>

            <!-- finally, the content of the page.-->
            <td class="maincontent">

              <div
                style="margin-top: 8px; float:left; overflow: scroll; width: 100%; height: 100%; overflow-x: hidden; overflow-y: auto;">

                <p>
                  I am a roboticist with specialization in 3D computer vision. I was recently
                  founder/indiehacker of MassSim, which focused on creating an interactive simulator for digital humans.
                  Previously up to
                  2023 I was at TuSimple, where I created pose estimation, state estimation, sensor calibration, and
                  mapping
                  technology. Before that up to 2019 I built 360 degree image capture technology at Fyusion. I have been
                  a
                  part of multiple academic labs and internships. I obtained my Ph.D. from the Georgia Institute of
                  Technology in Fall 2019. I received my M.S. and B.S. in 2011 and 2008, respectively from Iowa State
                  University. <br>
                  <br>
                <p>
                  <br>
                <h5>DEMOS</h5>
                <ul>
                  <div style="text-align:right; float:right;">(2024)&nbsp;</div>
                  <a href="https://mass-sim.com">HairSim v0.3.7</a><br>
                  <a
                    href="https://www.reddit.com/r/Simulated/comments/1e57ity/worlds_first_interactive_hair_sim_with_a/">World's
                    first interactive hair sim with a fine-tooth comb!</a><br>
                  <i>MassSim</i>
                  <p>
                  <div style="text-align:right; float:right;">(2021)&nbsp;</div>
                  <a href="https://www.youtube.com/watch?v=dGglN4J9zZ0">World's First "Driver Out" Fully Autonomous
                    Semi-Truck Operating on Open Public Roads.</a><br>
                  <i>TuSimple</i>
                  <p>
                  <div style="text-align:right; float:right;">(2020)&nbsp;</div>
                  <a href="https://vimeo.com/446861900">Fyusion 360 Image Capture</a>
                  <br>
                  <i>Fyusion</i>
                </ul>
                <p>
                  <br>
                <h5>[<span class="summary-link" data-target="full-publication-list">+</span>] SELECTED PUBLICATIONS</h5>
                <table class="background" width="100%" border="0" cellspacing="0" cellpadding="0">
                  <tr>
                    <td width="40" style="vertical-align: top; text-align: center;">[<span class="summary-link"
                        data-target="summary0">+</span>]</td>
                    <td width="552">
                      <div style="text-align:right; float:right;">(2024)&nbsp;</div>
                      <a href="https://www.freepatentsonline.com/y2024/0249435.html">Non-Player Character Visual
                        Odometry</a><br>
                      [<a href="files/2024_patent_griffith_NPC_VO_slides.pdf">slides</a>] <br>
                      Shane Griffith.<br>
                      <i>TuSimple.</i><br>
                      <div id="summary0" style="display: none;"><br>
                        <hr>
                        In 2022 at TuSimple, I discovered a visual odometry method for Advanced Driver Assistance
                        Systems (ADAS), achieving
                        pitch and yaw accuracy better than 0.2 degrees without relying on a map. Traditional pose
                        estimation systems depend on
                        static road features like lane lines and traffic signs, which often fail under challenging
                        conditions such as sun glare
                        or construction zones. I recognized that other vehicles, despite their dynamic nature, can be
                        leveraged for pose
                        estimation because they are near the plane-at-infinity with nearly zero relative translation. By
                        deriving a model that
                        relates their motion to the vehicle's pose, I introduced a new foundational information source
                        for ADAS. This model not
                        only enhances pose estimation reliability in difficult environments but also has broad
                        applications in alignment
                        verification, online calibration, and tracking.
                        <hr><br>
                      </div>
                      <p>
                    </td>
                  </tr>

                  <tr>
                    <td width="40" style="vertical-align: top; text-align: center;">[<span class="summary-link"
                        data-target="summary1">+</span>]</td>
                    <td width="552">
                      <div style="text-align:right; float:right;">(2019)&nbsp;</div>
                      <a href="research/2019_IJRR_griffith_et_al.pdf">Transforming Multiple Visual
                        Surveys of a Natural Environment Into Time-Lapses.</a>
                      <br>
                      [<a href="files/2019_dissertation_griffith_et_al_slides.pdf">slides</a>]
                      [<a href="2019_dissertation_griffith_demo.mp4">video</a>]
                      [<a href="https://dream.georgiatech-metz.fr/datasets/">dataset</a>]
                      <br>
                      Shane Griffith, Frank Dellaert, and Cedric Pradalier.<br>
                      <i>International Journal of Robotics Research (IJRR).</i> <br>
                      <p>
                      <div style="text-align:right; float:right;">(2016)&nbsp;</div>
                      <a href="research/2016_BMVC_griffith_pradalier.pdf">Reprojection Flow for Image Registration
                        Across
                        Seasons.</a>
                      <br>
                      Shane Griffith and Cedric Pradalier.<br>
                      <i>British Machine Vision Conference (BMVC)</i>, York, UK. <br>
                      <div id="summary1" style="display: none;">
                        <br>
                        <hr>
                        I completed my PhD during a time of rapid churn; deep learning quickly overtook most areas of AI
                        as the
                        state-of-the-art. Yet, I'd already undertaken work on a field robot, building on well-known,
                        classic perception
                        algorithms. We were trying to solve the hard problem of visual data association across
                        seasons in a natural
                        environment. A recurring question was whether and how much deep learning I should use. It
                        turned out that 3D visual geometry, as we can get with classic techniques, was
                        a key source of
                        information for our problem. This was supported by some nice results from biology as well.
                        <p>
                          And such was the backdrop that set the stage to confront the limitations of vision in natural
                          environments. The outlook
                          became clearer, as I articulated in my dissertation:
                        <p>
                          Vision is one of the primary sensory modalities of animals and robots, yet among robots it
                          still
                          has
                          limited power in natural environments. Dynamic processes of Nature continuously change how an
                          environment looks, which work against appearance-based methods for visual data association. As
                          a
                          robot is deployed again and again, the possibility of finding correspondences diminishes
                          between
                          surveys increasingly separated in time. This is a major limitation of intelligent systems
                          targeted
                          for precision agriculture, search and rescue, and environment monitoring. We
                          sought a new approach to visual data association to overcome the variation in appearance of a
                          natural environment, as it was experienced by a field robot over several years.

                        <p>
                          We found success with a map-centric approach, which builds on 3D vision to achieve visual
                          data association across seasons. We first created the Symphony Lake Dataset, which consists
                          of fortnightly visual surveys of a 1.3 km lakeshore captured from an autonomous surface
                          vehicle
                          over three years. We then established dense correspondence as a technique to both provide
                          robust
                          visual data association and to eliminate the variation in viewpoint between surveys. Given a
                          consistent map and localized poses, we next found that we could achieve visual data
                          association across seasons by integrating map point priors and geometric constraints into the
                          dense correspondence
                          image
                          alignment optimization. We called this algorithm "Reprojection Flow".

                        <p>
                          We presented the first work to see through the variation in appearance across seasons in
                          a natural environment using map point priors and localized poses. Our algorithm for
                          map-anchored
                          dense correspondence showed a substantial gain in visual data association in the midst of the
                          difficult variation in appearance. Up to 37 surveys were transformed into year-long
                          time-lapses at
                          the scenes where their maps were consistent. This indicates that, at a time when frequent
                          advancements are made using deep learning towards robust visual data association, the spatial
                          information in a map may
                          be able to close the distance where hard cases have persisted between observations.
                          <hr><br>
                      </div>
                      <p>
                    </td>
                  </tr>

                  <tr>
                    <td width="40" style="vertical-align: top; text-align: center;">[<span class="summary-link"
                        data-target="summary2">+</span>]</td>
                    <td width="552">
                      <div style="text-align:right; float:right;">(2013)&nbsp;</div>
                      <a href="research/2013_NIPS_griffith_et_al.pdf">Policy Shaping: Integrating Human Feedback with
                        Reinforcement
                        Learning.</a> <br>
                      [<a href="files/2013_NIPS_griffith_et_al_slides.pdf">slides</a>]
                      [<a href="research/2013_NIPS_griffith_et_al_code.py">code</a>]
                      [<a href="research/2013_NIPS_griffith_et_al_supplemental.pdf">appendix</a>]
                      <br>
                      Shane Griffith, Kaushik Subramanian, Jon Scholz, Charles Isbell, and Andrea Thomaz.<br>
                      <i>Advances in Neural Information Processing Systems (NeurIPS)</i>, 2625-2633, Lake Tahoe, Nevada.
                      <br>
                      <div id="summary2" style="display: none;">
                        <br>

                        <hr>
                        I passed the Qual after investigating how robots might learn and explore without
                        being labeled defective.
                        Although a large body of work already addresses how a robot might explore its environment in
                        order
                        to learn and adapt to it, the risks of exploration are commonly overlooked. Few papers addressed
                        how
                        a robot could reliably stay out of harm's way if it is left to its own devices. After I saw this
                        problem, my research goal was to investigate how robots could avoid committing serious errors
                        during their exploratory learning phase.

                        <p>
                          A step towards error free learning is made possible with a new insight about how to learn from
                          human feedback. Feedback interpreted as a direct label on the correctness of an action can
                          provide
                          a way to eliminate hazardous sections of the state space. This is in contrast to most previous
                          work in which feedback is interpreted as a reward (e.g., reward shaping), which creates
                          something
                          like a trail of breadcrumbs for coaxing an agent out of an undesirable or dangerous area. Our
                          new
                          ``policy shaping'' approach to interactive machine learning called for a fundamentally new way
                          to
                          use feedback with reinforcement learning.

                        <p>
                          We ended up deriving a simple, yet rigorous, information theoretic algorithm to maximize the
                          information gained from human feedback, which we named <b>Advise</b>. Our experiments showed
                          <b>Advise</b> in some cases significantly outperformed state-of-the-art methods, and was
                          robust to
                          noise. It also eliminates the ad hoc parameter tuning common of methods that interpret
                          feedback as
                          a reward. These advancements were presented at the 1st Biennial Conference on
                          Reinforcement Learning and Decision Making (RLDM), where it was one of the top four papers,
                          and
                          published in the 27th Annual Conference on Neural Information Processing Systems (NeurIPS).

                          <hr>
                          <br>
                      </div>
                      <p>

                    </td>
                  </tr>


                  <tr>
                    <td width="40" style="vertical-align: top; text-align: center;">[<span class="summary-link"
                        data-target="summary3">+</span>]</td>
                    <td width="552">
                      <div style="text-align:right; float:right;">(2012)&nbsp;</div>
                      <a href="research/2012_TAMD_griffith_et_al.pdf">A Behavior-Grounded Approach to
                        Forming Object Categories:<br> Separating Containers from Non-Containers.</a>
                      <br>
                      [<a href="files/2012_ICRA_SPME_griffith_et_al_slides.pdf">slides</a>]
                      [<a href="https://www.youtube.com/watch?v=xvEJdMFx1to">videos</a>]
                      <br>
                      Shane Griffith, Jivko Sinapov, Vlad Sukhoy, and Alex Stoytchev,<br>
                      <i>IEEE Transactions on Autonomous Mental Development (TAMD)</i>, 4:1, 54-69. <br>
                      <div id="summary3" style="display: none;">
                        <br>
                        <hr>
                        I earned my M.S. after 3.5 years of studying what a container is, and how a humanoid robot can
                        learn
                        what a container is. Although a growing body of literature in robotics addressed many different
                        container manipulation problems, individual papers only chipped away at isolated problems one by
                        one. This meant that the algorithms for one domain were not directly applicable to other
                        domains.
                        After I saw this problem, the goal of my thesis was to identify how a robot could start to learn
                        about containers in a more general way.

                        <p>
                          Because people have a representation of containers that is generalizable across many different
                          container manipulation problems, I looked to psychology for all the information on the origins
                          of
                          container learning. Psychologists observed that infants form an abstract spatial category for
                          containers, which allows them to apply their knowledge to novel containers. At the time,
                          however,
                          the current theories of object categorization weren't clear about exactly how infants form an
                          object category for containers. Consequently, I looked more deeply into the psychology
                          literature
                          in order to try and understand how infants learn.

                        <p>
                          By citing many different theories and observations from psychology, I extrapolated an
                          explanation
                          for how infants learn object categories. As a result of the expertise of the whole team, we
                          were
                          able to create a computational framework for learning object categories in a similar way by a
                          robot. Our experiments with containers showed that this method of object categorization really
                          works, and it works really well.

                        <p>

                          Our work was well received when we submitted it to the IEEE Transactions on Autonomous Mental
                          Development (TAMD) for publication in their journal. An eminent developmental psychologist
                          reviewed the object categorization theory (the expertise of
                          the other two reviewers was robotics), and in her "comments to the author" that we received
                          when
                          the paper was accepted, she signed her name in her review (reviews are usually anonymous) and
                          said:
                        <blockquote> "I commend the authors on a fantastic literature review of my domain. The authors
                          accurately cite a broad array of the relevant literature. There were no relevant articles
                          missing.
                          I do not have any suggested changes because I think the literature is very good as it is. ...I
                          was
                          tickled by the unification of citations from people that are
                          often perceived to be in opposing theoretical camps. ...I signed this review because I hope
                          that
                          the authors send me a copy when they get it published. I find the work fascinating and I would
                          like to refer to their in my own work."
                        </blockquote> In addition to technical comments that helped us to improve our work, the two
                        roboticists said "[this paper presents] an interesting and out-of-the-box way of addressing
                        concept
                        acquisition" and "this paper makes a significant contribution to the existing literature." In
                        the
                        end, my research productivity for my M.S. came to rest at
                        <font face="Times">&#960;</font> (11 papers in 3.5 years). <p>
                          <hr><br>
                      </div>
                    </td>
                  </tr>
                </table>
                <div id="full-publication-list" style="display: none;">
                  <br>
                  <hr>
                  <h5>FULL PUBLICATION LIST</h5>

                  <font class="maincontent" face="Baskerville" style="font-size:14px">Patents</font>
                  <p>
                  <p>
                  <ul>
                    <div style="text-align:right; float:right;">(2024)&nbsp;</div>
                    <a href="https://www.freepatentsonline.com/y2024/0249435.html">Non-Player Character Visual
                      Odometry</a><br>
                    [<a href="files/2024_patent_griffith_NPC_VO_slides.pdf">slides</a>] <br>
                    Shane Griffith.<br>
                    <i>TuSimple.</i><br>
                    <p>

                    <div style="text-align:right; float:right;">(2023)&nbsp;</div>
                    <a href="https://patents.google.com/patent/US20220408019A1/en">In-Situ Multi-Camera Extrinsic
                      Calibration</a><br>
                    [<a href="files/2020_patent_griffith_et_al_slides.pdf">slides</a>] <br>
                    Shane Griffith, Chenghao Gong, and Chenzhe Qian.<br>
                    <i>TuSimple.</i><br>
                    <p>

                    <div style="text-align:right; float:right;">(2021)&nbsp;</div>
                    <a href="https://patents.google.com/patent/US20220408019A1/en">Viewpoint path modeling.</a><br>
                    [<a href="files/2018_patent_chande_et_al_slides.pdf">slides</a>] <br>
                    Krunal Chande, Stefan Josef Holzer, Wook Yeon Hwang, Alexander Trevor, and Shane Griffith. <br>
                    <i>Fyusion.</i><br>
                    <p>
                  </ul>

                  <font class="maincontent" face="Baskerville" style="font-size:14px">Dissertation and Thesis</font>
                  <p>
                  <p>
                  <ul>
                    <div style="text-align:right; float:right;">(2019)&nbsp;</div>
                    <a href="research/2019_dissertation_griffith.pdf">Map-centric visual data association across seasons
                      in a
                      natural environment.</a> <br>
                    Ph.D. Dissertation. Georgia Institute of Technology. <br>
                    <p>
                    <div style="text-align:right; float:right;">(2011)&nbsp;</div>
                    <a href="research/2011_thesis_griffith.pdf">Separating containers from non-containers: <br>A
                      framework
                      for
                      learning behavior-grounded object categories.</a> <br>
                    M.S. Thesis. Iowa State University. <br>
                    <p>
                  </ul>


                  <font face="Baskerville" style="font-size:14px">Journals/Conferences/Workshops</font>
                  <p>
                  <p>
                  <ul>
                    <p>
                    <div style="text-align:right; float:right;">(2019)&nbsp;</div>
                    <a href="research/2019_IJRR_griffith_et_al.pdf">Transforming Multiple Visual Surveys of a Natural
                      Environment
                      Into
                      Time-Lapses.</a>
                    <br> [<a href="2019_dissertation_griffith_demo.mp4">video</a>] <br>
                    Shane Griffith, Frank Dellaert, and Cedric Pradalier.<br>
                    <i>International Journal of Robotics Research (IJRR)</i><br>
                    <p>
                    <div style="text-align:right; float:right;">(2017)&nbsp;</div>
                    <a href="research/2017_IJRR_griffith_et_al.pdf">Symphony Lake Dataset.</a><br>
                    [<a href="http://dream.georgiatech-metz.fr/?q=node/76">files</a>]<br>
                    Shane Griffith, Georges Chahine, and Cedric Pradalier.<br>
                    <i>International Journal of Robotics Research (IJRR)</i>, 36, 1151-1158.<br>
                    <p>
                    <div style="text-align:right; float:right;">(2016)&nbsp;</div>
                    <a href="research/2016_BMVC_griffith_pradalier.pdf">Reprojection Flow for Image Registration Across
                      Seasons.</a> <br>
                    Shane Griffith and Cedric Pradalier.<br>
                    <i>British Machine Vision Conference (BMVC)</i>, York, UK.<br>
                    <p>
                    <div style="text-align:right; float:right;">(2016)&nbsp;</div>
                    <a href="research/2016_JFR_griffith_pradalier.pdf">Survey Registration for Long-Term Natural
                      Environment
                      Monitoring.</a> <br>
                    Shane Griffith and Cedric Pradalier.<br>
                    <i>Journal of Field Robotics</i><br>
                    <p>
                    <div style="text-align:right; float:right;">(2015)&nbsp;</div>
                    <a href="research/2015_FSR_griffith_pradalier.pdf"> A Spatially and Temporally Scalable Approach for
                      Long-Term
                      Lakeshore Monitoring.</a> <br>
                    Shane Griffith and Cedric Pradalier. <br>
                    <i>Field and Service Robotics (FSR)</i>, Toronto, Canada.<br>
                    <p>
                    <div style="text-align:right; float:right;">(2015)&nbsp;</div>
                    <a href="research/2015_MVIGRO_griffith_et_al.pdf"> Robot-Enabled Lakeshore Monitoring Using
                      Visual SLAM
                      and
                      SIFT Flow.</a> <br>
                    Shane Griffith, Frank Dellaert, and Cedric Pradalier. <br>
                    <i>RSS Workshop on Multi-View Geometry in Robotics</i>, Rome, Italy.<br>
                    <p>
                    <div style="text-align:right; float:right;">(2014)&nbsp;</div>
                    <a href="research/2014_ISER_griffith_et_al.pdf">Towards Autonomous Lakeshore Monitoring.</a> <br>
                    Shane Griffith, Paul Drews, and Cedric Pradalier. <br>
                    <i>International Symposium on Experimental Robotics (ISER)</i>, Marrakech, Morocco. <br>
                    <p>
                    <div style="text-align:right; float:right;">(2013)&nbsp;</div>
                    <a href="research/2013_NIPS_griffith_et_al.pdf">Policy Shaping: Integrating Human Feedback with
                      Reinforcement
                      Learning.</a><br>
                    [<a href="research/2013_NIPS_griffith_et_al_supplemental.pdf">appendix</a>] [<a
                      href="research/2013_NIPS_griffith_et_al_code.py">code</a>] <br>
                    Shane Griffith, Kaushik Subramanian, Jon Scholz, Charles Isbell, and Andrea Thomaz.<br>
                    <i>Advances in Neural Information Processing Systems (NIPS)</i>, 2625-2633, Lake Tahoe, Nevada. <br>
                    <p>
                    <div style="text-align:right; float:right;">(2012)&nbsp;</div>
                    <a href="research/2011_ICRA_SPME_griffith_et_al.pdf">Object Categorization in the Sink: Learning
                      Behavior--Grounded
                      Object Categories With Water</a><br>
                    Shane Griffith, Vlad Sukhoy, Todd Wegter, and Alex Stoytchev.<br>
                    <i>ICRA Workshop on SPME</i>, St. Paul, Minnesota.<br>
                    <p>
                    <div style="text-align:right; float:right;">(2012)&nbsp;</div>
                    <a href="research/2012_TAMD_griffith_et_al.pdf">A Behavior-Grounded Approach to
                      Forming
                      Object Categories:<br> Separating Containers from Non-Containers.</a> <br>
                    Shane Griffith, Jivko Sinapov, Vlad Sukhoy, and Alex Stoytchev,<br>
                    <i>IEEE Transactions on Autonomous Mental Development (TAMD)</i>, 4:1, 54-69. <br>
                    <p>
                    <div style="text-align:right; float:right;">(2011)&nbsp;</div>
                    <a href="research/2011_Humanoids_griffith_et_al.pdf">Using Sequences of Movement Dependency Graphs
                      to Form
                      Object
                      Categories.</a> <br>
                    Shane Griffith, Vlad Sukhoy, and Alex Stoytchev.<br>
                    <i>Humanoids</i>, 715-720, Bled, Slovenia. <br>
                    <p>
                    <div style="text-align:right; float:right;">(2011)&nbsp;</div>
                    <a href="research/2011_RSS_WSIL_sukhoy_et_al.pdf"> Toward Imitating Object Manipulation Tasks Using
                      Sequences
                      of Movement Dependency Graphs.</a> <br>
                    Vlad Sukhoy, Shane Griffith, and Alex Stoytchev.<br>
                    <i>RSS Workshop on The State of Imitation Learning</i>, Los Angeles, California. <br>
                    <p>
                    <div style="text-align:right; float:right;">(2011)&nbsp;</div>
                    <a href="research/2011_IJRR_sinapov_et_al.pdf">Interactive Object Recognition Using Proprioceptive
                      and
                      Auditory Feedback.</a><br>
                    Jivko Sinapov, Taylor Bergquist, Connor Schenck, Ugonna Ohiri, Shane Griffith, and Alex
                    Stoytchev
                    <br>
                    <i>International Journal of Robotics Research (IJRR)</i>, 30:1, 1250-1262.
                    <p>
                    <div style="text-align:right; float:right;">(2010)&nbsp;</div>
                    <a href="research/2010_AAAI_griffith_et_al.pdf">Interactive Categorization of Containers and
                      Non-Containers by Unifying Categorizations Derived From Multiple Exploratory Behaviors.</a>
                    <br>
                    Shane Griffith and Alex Stoytchev.<br>
                    <i>Association for the Advancement of Artificial Intelligence (AAAI)</i>, Atlanta, Georgia.<br>
                    <p>
                    <div style="text-align:right; float:right;">(2010)&nbsp;</div>
                    <a href="research/2010_ICRA_griffith_et_al.pdf">How to Separate Containers from Non-Containers?<br>
                      A
                      Behavior-Grounded Approach to Acoustic Object Categorization.</a> <br>
                    Shane Griffith, Jivko Sinapov, Vlad Sukhoy, and Alex Stoytchev.<br>
                    <i>IEEE International Conference on Robotics and Automation (ICRA)</i>, 1852-1859, Anchorage,
                    Alaska.
                    <br>
                    <p>
                    <div style="text-align:right; float:right;">(2009)&nbsp;</div>
                    <a href="research/2009_IROS_bergquist_et_al.pdf">Interactive Object Recognition Using Proprioceptive
                      Feedback.</a><br>
                    Taylor Bergquist, Connor Schenck, Ugonna Ohiri, Jivko Sinapov, Shane Griffith, and Alex
                    Stoytchev<br>
                    <i>IROS Workshop on Semantic Perception for Mobile Manipulation</i>, St. Louis, Missouri. <br>
                    <p>
                    <div style="text-align:right; float:right;">(2009)&nbsp;</div>
                    <a href="research/2009_RSS_sahai_et_al.pdf">Interactive Identification of Writing Instruments and
                      Writable Surfaces by a robot.</a> <br>
                    Ritika Sahai, Shane Griffith, and Alex Stoytchev.<br>
                    <i>RSS Workshop on Mobile Manipulation in Human Environments</i>, Seattle, Washington. <br>
                    <p>
                    <div style="text-align:right; float:right;">(2009)&nbsp;</div>
                    <a href="research/2009_ICDL_griffith_et_al.pdf"> Toward Interactive Learning of Object Categories
                      by a
                      Robot:<br> A Case Study with Container and Non-Container Objects.</a> <br>
                    Shane Griffith, Jivko Sinapov, Matt Miller, and Alex Stoytchev.<br>
                    <i>8th IEEE International Conference on Development and Learning (ICDL)</i>, Shanghai, China. <br>
                  </ul>

                  <p>
                    <font face="Baskerville" style="font-size:14px">Accepted Abstracts/Presentations</font>
                  <p>
                  <ul>
                    <div style="text-align:right; float:right;">(2016)&nbsp;</div>
                    <a href="research/2016_AILTA_griffith_pradalier.pdf">Towards Reprojection Flow for Image
                      Registration
                      Across Seasons.</a><br>
                    Shane Griffith and Cedric Pradalier.<br>
                    <i>ICRA Workshop on AI for Long-Term Autonomy</i>, Stockholm, Sweden.<br>
                    <p>
                    <div style="text-align:right; float:right;">(2013)&nbsp;</div>
                    <a href="research/2013_RLDM_griffith_et_al.pdf">Policy Shaping: Integrating Human Feedback with
                      Reinforcement
                      Learning.</a><br>
                    Shane Griffith, Kaushik Subramanian, Jon Scholz, Charles Isbell, and Andrea Thomaz.<br>
                    <i>1st Multidisciplinary Conference on Reinforcement Learning and Decision Making (RLDM)</i>,
                    Princeton,
                    New Jersey.<br>
                    <p>
                    <div style="text-align:right; float:right;">(2009)&nbsp;</div>
                    <a href="research/2009_ICDL_sahai_et_al.pdf">Toward Learning to Write by Identifying Writable
                      Surfaces.</a> <br>
                    Ritika Sahai, Shane Griffith, and Alex Stoytchev.<br>
                    <i>8th IEEE International Conference on Development and Learning (ICDL)</i>, Poster Abstract,
                    Shanghai,
                    China.<br>
                    <p>
                    <div style="text-align:right; float:right;">(2009)&nbsp;</div>
                    <a href="research/2009_HRI_griffith_et_al.pdf">Learning to Detect Containers with Human
                      Assistance.</a><br>
                    Shane Griffith.<br>
                    <i>HRI Pioneers Workshop</i>, Workshop Abstract, San Diego, California.<br>
                    <p>
                    <div style="text-align:right; float:right;">(2008)&nbsp;</div>
                    <a href="research/2008_ICDL_griffith_et_al.pdf">Toward Learning to Detect and Use
                      Containers.</a><br>
                    Shane Griffith, Jivko Sinapov, and Alex Stoytchev.<br>
                    <i>7th IEEE International Conference on Development and Learning (ICDL)</i>, Poster Abstract,
                    Monterey,
                    California.<br>
                    <p>
                    <div style="text-align:right; float:right;">(2008)&nbsp;</div>
                    <a href="">Holonomic Architecture for Networked Cooperative Robots.</a><br>
                    Alex Baumgarten, John Dashner, Shane Griffith, Kyle Miller, Mark Rabe, Chris Tott, Jon Watson,
                    Joshua Watt, and Nicola Elia.<br>
                    <i>IEEE International Conference on Electro/Information Technology (EIT)</i>, Undergraduate Student
                    Paper Competition, Ames, Iowa. <br>
                    <p>
                    <div style="text-align:right; float:right;">(2009, 2010, 2011)&nbsp;</div>
                    <i>ISU ETC/WINVR</i>
                    <p>
                    <div style="text-align:right; float:right;">(2007, 2008)&nbsp;</div>
                    <i>ISU Undergraduate Research Symposium</i><br>
                  </ul>
                  <hr>
                </div>
            </td>
          </tr>
        </table>

        <!-- adds space between the content and the footer-->
        <p>&nbsp;</p>

      </td>
    </tr>


    <!-- adds space between the content and the footer-->
    <p>&nbsp;</p>

    </td>
    </tr>

  </table>
  <!-- adds space between the footer and the end of the page-->
  <p>&nbsp;</p>

  <script>
    // Add event listeners to all summary links
    document.querySelectorAll('.summary-link').forEach(function (link) {
      link.addEventListener('click', function (e) {
        e.preventDefault();
        var targetId = e.target.getAttribute('data-target');
        var target = document.getElementById(targetId);
        if (target.style.display === 'none') {
          target.style.display = 'block';
          e.target.textContent = '-';
        } else {
          target.style.display = 'none';
          e.target.textContent = '+';
        }
      });
    });
  </script>

</body>

</html>